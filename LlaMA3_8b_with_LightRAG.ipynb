{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm_vZcV1w0Iq",
        "outputId": "c54e55d7-7b42-49a0-dd18-c5bf76ff0af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightrag[faiss-cpu,groq,openai]\n",
            "  Downloading lightrag-0.1.0b2-py3-none-any.whl (152 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/152.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m143.4/152.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.7/152.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff<3.0.0,>=2.2.1 (from lightrag[faiss-cpu,groq,openai])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from lightrag[faiss-cpu,groq,openai]) (3.1.4)\n",
            "Collecting jsonlines<5.0.0,>=4.0.0 (from lightrag[faiss-cpu,groq,openai])\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting numpy<2.0.0,>=1.26.4 (from lightrag[faiss-cpu,groq,openai])\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv<2.0.0,>=1.0.1 (from lightrag[faiss-cpu,groq,openai])\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from lightrag[faiss-cpu,groq,openai]) (6.0.1)\n",
            "Collecting tiktoken<0.8.0,>=0.7.0 (from lightrag[faiss-cpu,groq,openai])\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.4 in /usr/local/lib/python3.10/dist-packages (from lightrag[faiss-cpu,groq,openai]) (4.66.4)\n",
            "Collecting groq<0.6.0,>=0.5.0 (from lightrag[faiss-cpu,groq,openai])\n",
            "  Downloading groq-0.5.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu<2.0.0,>=1.8.0 (from lightrag[faiss-cpu,groq,openai])\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai<2.0.0,>=1.12.0 (from lightrag[faiss-cpu,groq,openai])\n",
            "  Downloading openai-1.35.14-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu<2.0.0,>=1.8.0->lightrag[faiss-cpu,groq,openai]) (24.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai])\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.3->lightrag[faiss-cpu,groq,openai]) (2.1.5)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines<5.0.0,>=4.0.0->lightrag[faiss-cpu,groq,openai]) (23.2.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[faiss-cpu,groq,openai]) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[faiss-cpu,groq,openai]) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai])\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m262.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[faiss-cpu,groq,openai]) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[faiss-cpu,groq,openai]) (2.0.7)\n",
            "Installing collected packages: python-dotenv, numpy, jsonlines, h11, backoff, tiktoken, httpcore, faiss-cpu, lightrag, httpx, openai, groq\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "Successfully installed backoff-2.2.1 faiss-cpu-1.8.0.post1 groq-0.5.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonlines-4.0.0 lightrag-0.1.0b2 numpy-1.26.4 openai-1.35.14 python-dotenv-1.0.1 tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install lightrag[openai,groq,faiss-cpu]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U lightrag[openai,groq,faiss-cpu]\n",
        "!pip show lightrag openai groq faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGT4Y7M0xONv",
        "outputId": "dbf80e36-4523-42b8-84e9-55a96107e897"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightrag[faiss-cpu,groq,openai] in /usr/local/lib/python3.10/dist-packages (0.1.0b2)\n",
            "Requirement already satisfied: backoff<3.0.0,>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from lightrag[faiss-cpu,groq,openai]) (2.2.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from lightrag[faiss-cpu,groq,openai]) (3.1.4)\n",
            "Requirement already satisfied: jsonlines<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightrag[faiss-cpu,groq,openai]) (4.0.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.4 in /usr/local/lib/python3.10/dist-packages (from lightrag[faiss-cpu,groq,openai]) (1.26.4)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lightrag[faiss-cpu,groq,openai]) (1.0.1)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from lightrag[faiss-cpu,groq,openai]) (6.0.1)\n",
            "Requirement already satisfied: tiktoken<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightrag[faiss-cpu,groq,openai]) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.4 in /usr/local/lib/python3.10/dist-packages (from lightrag[faiss-cpu,groq,openai]) (4.66.4)\n",
            "Requirement already satisfied: groq<0.6.0,>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from lightrag[faiss-cpu,groq,openai]) (0.5.0)\n",
            "Requirement already satisfied: faiss-cpu<2.0.0,>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from lightrag[faiss-cpu,groq,openai]) (1.8.0.post1)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from lightrag[faiss-cpu,groq,openai]) (1.35.14)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu<2.0.0,>=1.8.0->lightrag[faiss-cpu,groq,openai]) (24.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.3->lightrag[faiss-cpu,groq,openai]) (2.1.5)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines<5.0.0,>=4.0.0->lightrag[faiss-cpu,groq,openai]) (23.2.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[faiss-cpu,groq,openai]) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[faiss-cpu,groq,openai]) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<0.6.0,>=0.5.0->lightrag[faiss-cpu,groq,openai]) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[faiss-cpu,groq,openai]) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[faiss-cpu,groq,openai]) (2.0.7)\n",
            "Name: lightrag\n",
            "Version: 0.1.0b2\n",
            "Summary: The Lightning Library for LLM Applications.\n",
            "Home-page: https://github.com/SylphAI-Inc/LightRAG\n",
            "Author: Li Yin\n",
            "Author-email: li@sylphai.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: backoff, jinja2, jsonlines, numpy, python-dotenv, pyyaml, tiktoken, tqdm\n",
            "Required-by: \n",
            "---\n",
            "Name: openai\n",
            "Version: 1.35.14\n",
            "Summary: The official Python library for the openai API\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: OpenAI <support@openai.com>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: anyio, distro, httpx, pydantic, sniffio, tqdm, typing-extensions\n",
            "Required-by: \n",
            "---\n",
            "Name: groq\n",
            "Version: 0.5.0\n",
            "Summary: The official Python library for the groq API\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Groq <support@groq.com>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: anyio, distro, httpx, pydantic, sniffio, typing-extensions\n",
            "Required-by: \n",
            "---\n",
            "Name: faiss-cpu\n",
            "Version: 1.8.0.post1\n",
            "Summary: A library for efficient similarity search and clustering of dense vectors.\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Kota Yamaguchi <yamaguchi_kota@cyberagent.co.jp>\n",
            "License: MIT License\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: numpy, packaging\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "groq_api = getpass(\"Please enter your groq API : \")\n",
        "openai_api = getpass(\"Please enter your openai API : \")\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api\n",
        "os.environ[\"GROQ_API_KEY\"] = groq_api\n",
        "\n",
        "print(\"API Keys has been set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tijZ-lZLxQpq",
        "outputId": "e7d17609-c9d8-49ca-c725-a278a4f4f6c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your groq API : ··········\n",
            "Please enter your openai API : ··········\n",
            "API Keys has been set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Dict\n",
        "\n",
        "from lightrag.core import Component, Generator, DataClass, ModelClient\n",
        "from lightrag.components.model_client import GroqAPIClient\n",
        "from lightrag.components.output_parsers import JsonOutputParser"
      ],
      "metadata": {
        "id": "gZow6qPK8MQE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class QAOutput(DataClass):\n",
        "  explanation : str = field(\n",
        "      metadata = {\"desc\": \"A brief explanation of the concept in one sentence.\"}\n",
        "  )\n",
        "  example : str = field(metadata={\"desc\": \"An example of the concept in a sentence.\"})\n",
        "\n",
        "\n",
        "\n",
        "qa_template = r\"\"\"<SYS>\n",
        "you are a helpful asistant.\n",
        "<OUTPUT_FORMAT>\n",
        "{{output_format_str}}\n",
        "</OUTPUT_FOMAT>\n",
        "</SYS>\n",
        "Uer: {{input_str}}\n",
        "You:\"\"\""
      ],
      "metadata": {
        "id": "Lie0SBUD87B0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the task pipeline\n",
        "\n",
        "class QA(Component):\n",
        "    def __init__(self, model_client: ModelClient, model_kwargs: Dict):\n",
        "        super().__init__()\n",
        "\n",
        "        parser = JsonOutputParser(data_class=QAOutput, return_data_class=True)\n",
        "        self.generator = Generator(\n",
        "            model_client=model_client,\n",
        "            model_kwargs=model_kwargs,\n",
        "            template=qa_template,\n",
        "            prompt_kwargs={\"output_format_str\": parser.format_instructions()},\n",
        "            output_processors=parser,\n",
        "        )\n",
        "\n",
        "    def call(self, query: str):\n",
        "        return self.generator.call({\"input_str\": query})\n",
        "\n",
        "    async def acall(self, query: str):\n",
        "        return await self.generator.acall({\"input_str\": query})"
      ],
      "metadata": {
        "id": "0qwlWbQW9w5q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = QA(\n",
        "    model_client = GroqAPIClient(),\n",
        "    model_kwargs = {\"model\": \"llama3-8b-8192\"}\n",
        ")\n",
        "\n",
        "print(qa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWSnpRrQ-_ji",
        "outputId": "aed6d833-acea-489d-a7da-ba7d304c608d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QA(\n",
            "  (generator): Generator(\n",
            "    model_kwargs={'model': 'llama3-8b-8192'}, \n",
            "    (prompt): Prompt(\n",
            "      template: <SYS>\n",
            "      you are a helpful asistant.\n",
            "      <OUTPUT_FORMAT>\n",
            "      {{output_format_str}}\n",
            "      </OUTPUT_FOMAT>\n",
            "      </SYS>\n",
            "      Uer: {{input_str}}\n",
            "      You:, prompt_kwargs: {'output_format_str': 'Your output should be formatted as a standard JSON instance with the following schema:\\n```\\n{\\n    \"explanation\": \"A brief explanation of the concept in one sentence. (str) (required)\",\\n    \"example\": \"An example of the concept in a sentence. (str) (required)\"\\n}\\n```\\n-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\\n-Use double quotes for the keys and string values.\\n-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\\n-Follow the JSON formatting conventions.'}, prompt_variables: ['output_format_str', 'input_str']\n",
            "    )\n",
            "    (model_client): GroqAPIClient()\n",
            "    (output_processors): JsonOutputParser(\n",
            "      data_class=QAOutput, examples=None, exclude_fields=None, return_data_class=True\n",
            "      (output_format_prompt): Prompt(\n",
            "        template: Your output should be formatted as a standard JSON instance with the following schema:\n",
            "        ```\n",
            "        {{schema}}\n",
            "        ```\n",
            "        {% if example %}\n",
            "        Examples:\n",
            "        ```\n",
            "        {{example}}\n",
            "        ```\n",
            "        {% endif %}\n",
            "        -Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\n",
            "        -Use double quotes for the keys and string values.\n",
            "        -DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\n",
            "        -Follow the JSON formatting conventions., prompt_variables: ['schema', 'example']\n",
            "      )\n",
            "      (output_processors): JsonParser()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa(\"What is LLM?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9rhis12_VO-",
        "outputId": "b4aa6789-c1db-45bf-9621-9b9356f4be31"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GeneratorOutput(data=QAOutput(explanation='LLM stands for Large Language Model, a type of artificial intelligence that is trained on a massive dataset of text and can generate human-like responses to natural language inputs.', example='For example, an LLM like language model can be used to generate chatbot responses, write articles, or even compose music.'), error=None, usage=None, raw_response='```\\n{\\n    \"explanation\": \"LLM stands for Large Language Model, a type of artificial intelligence that is trained on a massive dataset of text and can generate human-like responses to natural language inputs.\",\\n    \"example\": \"For example, an LLM like language model can be used to generate chatbot responses, write articles, or even compose music.\"\\n}\\n```', metadata=None)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa.generator.print_prompt(\n",
        "    output_format_str = qa.generator.output_processors.format_instructions(),\n",
        "    input_str = \"What is LLM?\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxu-2FLW_mZm",
        "outputId": "4e9dfbab-678e-4d3d-96cb-07dd63858585"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "______________________\n",
            "<SYS>\n",
            "you are a helpful asistant.\n",
            "<OUTPUT_FORMAT>\n",
            "Your output should be formatted as a standard JSON instance with the following schema:\n",
            "```\n",
            "{\n",
            "    \"explanation\": \"A brief explanation of the concept in one sentence. (str) (required)\",\n",
            "    \"example\": \"An example of the concept in a sentence. (str) (required)\"\n",
            "}\n",
            "```\n",
            "-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\n",
            "-Use double quotes for the keys and string values.\n",
            "-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\n",
            "-Follow the JSON formatting conventions.\n",
            "</OUTPUT_FOMAT>\n",
            "</SYS>\n",
            "Uer: What is LLM?\n",
            "You:\n"
          ]
        }
      ]
    }
  ]
}